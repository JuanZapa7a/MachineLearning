{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Lesson 08: Profiling Python Code](https://machinelearningmastery.com/profiling-python-code/)\n",
    "\n",
    "Profiling is a technique to figure out how time is spent in a program. With\n",
    "these statistics, we can find the “hot spot” of a program and think about ways\n",
    "of improvement. Sometimes, a hot spot in an unexpected location may hint at a\n",
    "bug in the program as well.\n",
    "\n",
    "In this tutorial, we will see how we can use the profiling facility in Python.\n",
    "Specifically, you will see:\n",
    "\n",
    "*    How we can compare small code fragments using the timeit module\n",
    "*    How we can profile the entire program using the cProfile module\n",
    "*    How we can invoke a profiler inside an existing program\n",
    "*    What the profiler cannot do\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial is in four parts; they are:\n",
    "\n",
    "*    Profiling small fragments\n",
    "*    The profile module\n",
    "*    Using profiler inside code\n",
    "*    Caveats\n",
    "\n",
    "## Profiling Small Fragments\n",
    "\n",
    "When you are asked about the different ways of doing the same thing in Python,\n",
    "one perspective is to check which one is more efficient. In Python’s standard\n",
    "library, we have the timeit module that allows us to do some simple profiling.\n",
    "\n",
    "For example, to concatenate many short strings, we can use the `join()` function\n",
    "from strings or the + operator. So, how do we know which is faster? Consider the\n",
    "following Python code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longstr = \"\"\n",
    "for x in range(1000):\n",
    "  longstr += str(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a long string 012345.... in the variable longstr. An\n",
    "alternative way to write this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longstr = \"\".join([str(x) for x in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the two, we can do the following at the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python -m timeit 'longstr=\"\"' 'for x in range(1000): longstr += str(x)'\n",
    "!python -m timeit '\"\".join([str(x) for x in range(1000)])'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above commands are to load the timeit module and pass on a single line of\n",
    "code for measurement. In the first case, we have two lines of statements, and\n",
    "they are passed on to the timeit module as two separate arguments. In the same\n",
    "rationale, the first command can also be presented as three lines of statements\n",
    "(by breaking the for-loop into two lines), but the indentation of each line\n",
    "needs to be quoted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m timeit 'longstr=\"\"' 'for x in range(1000):' ' longstr += str(x)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of timeit is to find the best performance among multiple runs\n",
    "(default to be 5). Each run is to run the provided statements a few times (which\n",
    "is dynamically determined). The time is reported as the average to execute the\n",
    "statements once in the best run.\n",
    "\n",
    "While it is true that the join function is faster than the + operator for string\n",
    "concatenation, the timing above is not a fair comparison. It is because we use\n",
    "str(x) to make short strings on the fly during the loop. The better way to do\n",
    "this is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m timeit -s 'strings = [str(x) for x in range(1000)]' 'longstr=\"\"' 'for x in strings:' ' longstr += str(x)'\n",
    "!python -m timeit -s 'strings = [str(x) for x in range(1000)]' '\"\".join(strings)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -s option allows us to provide the “setup” code, which is executed before\n",
    "the profiling and not timed. In the above, we create the list of short strings\n",
    "before starting the loop. Hence the time to create those strings is not measured\n",
    "in the “per loop” timing. The above shows that the join() function is two orders\n",
    "of magnitude faster than the + operator. The more common use of the -s option is\n",
    "to import the libraries. For example, we can compare the square root function\n",
    "from Python’s math module from NumPy and use the exponential operator ** as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m timeit '[x**0.5 for x in range(1000)]'\n",
    "!python -m timeit -s 'from math import sqrt' '[sqrt(x) for x in range(1000)]'\n",
    "!python -m timeit -s 'from numpy import sqrt' '[sqrt(x) for x in range(1000)]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above produces the following measurement, which we see that math.sqrt() is\n",
    "fastest while numpy.sqrt() is slowest in this particular example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wonder why NumPy is the slowest, it is because NumPy is optimized for\n",
    "arrays. You will see its exceptional speed in the following alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m timeit -s 'import numpy as np; x=np.array(range(1000))' 'np.sqrt(x)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, you can also run timeit in Python code. For example, the\n",
    "following will be similar to the above but give you the total raw timing for\n",
    "each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "measurements = timeit.repeat('[x**0.5 for x in range(1000)]', number=10000)\n",
    "print(measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, each run is to execute the statement 10,000 times; the result is as\n",
    "follows. You can see the result of roughly 71 usec per loop in the best run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Profile Module\n",
    "\n",
    "Focusing on a statement or two for performance is from a microscopic\n",
    "perspective. Chances are, we have a long program and want to see what is causing\n",
    "it to run slow. That happens before we can consider alternative statements or\n",
    "algorithms.\n",
    "\n",
    "A program running slow can generally be due to two reasons: A part is running\n",
    "slow, or a part is running too many times, adding up and taking too much time.\n",
    "We call these “performance hogs” the hot spot. Let’s look at an example.\n",
    "Consider the following program that uses a hill-climbing algorithm to find\n",
    "hyperparameters for a perceptron model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually search perceptron hyperparameters for binary classification\n",
    "from numpy import mean\n",
    "from numpy.random import randn\n",
    "from numpy.random import rand\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# objective function\n",
    "def objective(X, y, cfg):\n",
    "\t# unpack config\n",
    "\teta, alpha = cfg\n",
    "\t# define model\n",
    "\tmodel = Perceptron(penalty='elasticnet', alpha=alpha, eta0=eta)\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\t# calculate mean accuracy\n",
    "\tresult = mean(scores)\n",
    "\treturn result\n",
    "\n",
    "# take a step in the search space\n",
    "def step(cfg, step_size):\n",
    "\t# unpack the configuration\n",
    "\teta, alpha = cfg\n",
    "\t# step eta\n",
    "\tnew_eta = eta + randn() * step_size\n",
    "\t# check the bounds of eta\n",
    "\tif new_eta <= 0.0:\n",
    "\t\tnew_eta = 1e-8\n",
    "\tif new_eta > 1.0:\n",
    "\t\tnew_eta = 1.0\n",
    "\t# step alpha\n",
    "\tnew_alpha = alpha + randn() * step_size\n",
    "\t# check the bounds of alpha\n",
    "\tif new_alpha < 0.0:\n",
    "\t\tnew_alpha = 0.0\n",
    "\t# return the new configuration\n",
    "\treturn [new_eta, new_alpha]\n",
    "\n",
    "# hill climbing local search algorithm\n",
    "def hillclimbing(X, y, objective, n_iter, step_size):\n",
    "\t# starting point for the search\n",
    "\tsolution = [rand(), rand()]\n",
    "\t# evaluate the initial point\n",
    "\tsolution_eval = objective(X, y, solution)\n",
    "\t# run the hill climb\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# take a step\n",
    "\t\tcandidate = step(solution, step_size)\n",
    "\t\t# evaluate candidate point\n",
    "\t\tcandidate_eval = objective(X, y, candidate)\n",
    "\t\t# check if we should keep the new point\n",
    "\t\tif candidate_eval >= solution_eval:\n",
    "\t\t\t# store the new point\n",
    "\t\t\tsolution, solution_eval = candidate, candidate_eval\n",
    "\t\t\t# report progress\n",
    "\t\t\tprint('>%d, cfg=%s %.5f' % (i, solution, solution_eval))\n",
    "\treturn [solution, solution_eval]\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
    "# define the total iterations\n",
    "n_iter = 100\n",
    "# step size in the search space\n",
    "step_size = 0.1\n",
    "# perform the hill climbing search\n",
    "cfg, score = hillclimbing(X, y, objective, n_iter, step_size)\n",
    "print('Done!')\n",
    "print('cfg=%s: Mean Accuracy: %f' % (cfg, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal output of the program will be printed first, and then the profiler’s\n",
    "statistics will be printed. From the first row, we see that the function\n",
    "`objective()` in our program has run 101 times and took 4.89 seconds. But these\n",
    "4.89 seconds are mostly spent on the functions it called, which the total time\n",
    "spent on that function is merely 0.001 seconds. The functions from dependent\n",
    "modules are also profiled. Hence you see a lot of NumPy functions above too.\n",
    "\n",
    "The above output is long and may not be useful to you as it can be difficult to\n",
    "tell which function is the hot spot. Indeed we can sort the above output. For\n",
    "example, to see which function is called the most number of times, we can sort\n",
    "by ncalls:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd20b80f3abe9c1782c169f75cb3aaf4fed19688fa30b13c4021414b02823d14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
