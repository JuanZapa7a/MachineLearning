{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A Gentle Introduction to Serialization for Python](https://machinelearningmastery.com/a-gentle-introduction-to-serialization-for-python/)\n",
    "\n",
    "Serialization refers to the process of converting a data object (e.g., Python\n",
    "objects, Tensorflow models) into a format that allows us to store or transmit\n",
    "the data and then recreate the object when needed using the reverse process of\n",
    "deserialization.\n",
    "\n",
    "There are different formats for the serialization of data, such as JSON, XML,\n",
    "HDF5, and Python’s pickle, for different purposes. JSON, for instance, returns a\n",
    "human-readable string form, while Python’s pickle library can return a byte\n",
    "array.\n",
    "\n",
    "In this post, you will discover how to use two common serialization libraries in\n",
    "Python to serialize data objects (namely pickle and HDF5) such as dictionaries\n",
    "and Tensorflow models in Python for storage and transmission.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "*    Serialization libraries in Python such as pickle and h5py\n",
    "*    Serializing objects such as dictionaries and Tensorflow models in Python\n",
    "*    How to use serialization for memoization to reduce function calls\n",
    "\n",
    "## Overview\n",
    "\n",
    "The tutorial is divided into four parts; they are:\n",
    "\n",
    "*   What is serialization, and why do we serialize?\n",
    "*    Using Python’s pickle library\n",
    "*    Using HDF5 in Python\n",
    "*    Comparison between different serialization methods\n",
    "\n",
    "## What Is Serialization, and Why Should We Care?\n",
    "\n",
    "Think about storing an integer; how would you store that in a file or transmit\n",
    "it? That’s easy! We can just write the integer to a file and store or transmit\n",
    "that file.\n",
    "\n",
    "But now, what if we think about storing a Python object (e.g., a Python\n",
    "dictionary or a Pandas DataFrame), which has a complex structure and many\n",
    "attributes (e.g., the columns and index of the DataFrame, and the data type of\n",
    "each column)? How would you store it as a file or transmit it to another\n",
    "computer?\n",
    "\n",
    "This is where serialization comes in!\n",
    "\n",
    "Serialization is the process of converting the object into a format that can be\n",
    "stored or transmitted. After transmitting or storing the serialized data, we are\n",
    "able to reconstruct the object later and obtain the exact same structure/object,\n",
    "which makes it really convenient for us to continue using the stored object\n",
    "later on instead of reconstructing the object from scratch.\n",
    "\n",
    "In Python, there are many different formats for serialization available. One\n",
    "common example of hash maps (Python dictionaries) that works across many\n",
    "languages is the JSON file format which is human-readable and allows us to store\n",
    "the dictionary and recreate it with the same structure. But JSON can only store\n",
    "basic structures such as a list and dictionary, and it can only keep strings and\n",
    "numbers. We cannot ask JSON to remember the data type (e.g., numpy float32 vs.\n",
    "float64). It also cannot distinguish between Python tuples and lists.\n",
    "\n",
    "More powerful serialization formats exist. In the following, we will explore two common serialization libraries in Python, namely pickle and h5py.\n",
    "\n",
    "## Using Python’s Pickle Library\n",
    "\n",
    "The `pickle` module is part of the Python standard library and implements methods\n",
    "to serialize (pickling) and deserialize (unpickling) Python objects.\n",
    "\n",
    "To get started with `pickle`, import it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, to serialize a Python object such as a dictionary and store the byte\n",
    "stream as a file, we can use pickle’s `dump()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"Hello\": \"World!\"}\n",
    "with open(\"test.pickle\", \"wb\") as outfile:\n",
    " \t# \"wb\" argument opens the file in binary mode\n",
    "\tpickle.dump(test_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The byte stream representing `test_dict` is now stored in the file\n",
    "`“test.pickle”`!\n",
    "\n",
    "To recover the original object, we read the serialized byte stream from the file\n",
    "using pickle’s `load()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.pickle\", \"rb\") as infile:\n",
    " \ttest_dict_reconstructed = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Only unpickle data from sources you trust, as it is possible for\n",
    "arbitrary malicious code to be executed during the unpickling process.\n",
    "\n",
    "Putting them together, the following code helps you to verify that pickle can\n",
    "recover the same object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written object {'Hello': 'World!'}\n",
      "Reconstructed object {'Hello': 'World!'}\n",
      "Reconstruction success\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "# A test object\n",
    "test_dict = {\"Hello\": \"World!\"}\n",
    " \n",
    "# Serialization\n",
    "with open(\"test.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(test_dict, outfile)\n",
    "print(\"Written object\", test_dict)\n",
    " \n",
    "# Deserialization\n",
    "with open(\"test.pickle\", \"rb\") as infile:\n",
    "    test_dict_reconstructed = pickle.load(infile)\n",
    "print(\"Reconstructed object\", test_dict_reconstructed)\n",
    " \n",
    "if test_dict == test_dict_reconstructed:\n",
    "    print(\"Reconstruction success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides writing the serialized object into a pickle file, we can also obtain the\n",
    "object serialized as a bytes-array type in Python using pickle’s `dumps()`\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_ba = pickle.dumps(test_dict)      # b'\\x80\\x04\\x95\\x15…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use pickle’s load method to convert from a bytes-array type\n",
    "back to the original object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_reconstructed_ba = pickle.loads(test_dict_ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful thing about pickle is that it can serialize almost any Python object,\n",
    "including user-defined ones, such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Data from reconstructed object: 1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "class NewClass:\n",
    "    def __init__(self, data):\n",
    "        print(data)\n",
    "        self.data = data\n",
    " \n",
    "# Create an object of NewClass\n",
    "new_class = NewClass(1)\n",
    " \n",
    "# Serialize and deserialize\n",
    "pickled_data = pickle.dumps(new_class)\n",
    "reconstructed = pickle.loads(pickled_data)\n",
    " \n",
    "# Verify\n",
    "print(\"Data from reconstructed object:\", reconstructed.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle can even serialize Python functions since functions are first-class objects in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "def test():\n",
    "    return \"Hello world!\"\n",
    " \n",
    "# Serialize and deserialize\n",
    "pickled_function = pickle.dumps(test)\n",
    "reconstructed_function = pickle.loads(pickled_function)\n",
    " \n",
    "# Verify\n",
    "print (reconstructed_function()) #prints “Hello, world!”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can make use of pickle to save our work. For example, a trained\n",
    "model from Keras or scikit-learn can be serialized by pickle and loaded later\n",
    "instead of re-training the model every time we use it. The following shows you\n",
    "how we can build a LeNet5 model to recognize the **MNIST handwritten digits**\n",
    "using Keras, then serialize the trained model using pickle. Afterward, we can\n",
    "reconstruct the model without training it again, and it should produce exactly\n",
    "the same result as the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 12:03:03.619798: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-20 12:03:06.815570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/targets/x86_64-linux/lib/\n",
      "2022-07-20 12:03:06.815659: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/targets/x86_64-linux/lib/\n",
      "2022-07-20 12:03:06.815713: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/targets/x86_64-linux/lib/\n",
      "2022-07-20 12:03:06.815764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/targets/x86_64-linux/lib/\n",
      "2022-07-20 12:03:06.815812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/targets/x86_64-linux/lib/\n",
      "2022-07-20 12:03:06.815860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/targets/x86_64-linux/lib/\n",
      "2022-07-20 12:03:06.816805: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-20 12:03:06.817442: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1577 - accuracy: 0.9538 - val_loss: 0.0748 - val_accuracy: 0.9757\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0666 - accuracy: 0.9795 - val_loss: 0.0481 - val_accuracy: 0.9848\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0518 - accuracy: 0.9842 - val_loss: 0.0476 - val_accuracy: 0.9847\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0387 - accuracy: 0.9880 - val_loss: 0.0406 - val_accuracy: 0.9870\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0341 - accuracy: 0.9890 - val_loss: 0.0474 - val_accuracy: 0.9856\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.0406 - val_accuracy: 0.9875\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0406 - val_accuracy: 0.9872\n",
      "Epoch 8/100\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9926"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    " \n",
    "# Load MNIST digits\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "# Reshape data to (n_samples, height, wiedth, n_channel)\n",
    "X_train = np.expand_dims(X_train, axis=3).astype(\"float32\")\n",
    "X_test = np.expand_dims(X_test, axis=3).astype(\"float32\")\n",
    " \n",
    "# One-hot encode the output\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    " \n",
    "# LeNet5 model\n",
    "model = Sequential([\n",
    "    Conv2D(6, (5,5), input_shape=(28,28,1), padding=\"same\", activation=\"tanh\"),\n",
    "    AveragePooling2D((2,2), strides=2),\n",
    "    Conv2D(16, (5,5), activation=\"tanh\"),\n",
    "    AveragePooling2D((2,2), strides=2),\n",
    "    Conv2D(120, (5,5), activation=\"tanh\"),\n",
    "    Flatten(),\n",
    "    Dense(84, activation=\"tanh\"),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    " \n",
    "# Train the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[earlystopping])\n",
    " \n",
    "# Evaluate the model\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    " \n",
    "# Pickle to serialize and deserialize\n",
    "pickled_model = pickle.dumps(model)\n",
    "reconstructed = pickle.loads(pickled_model)\n",
    " \n",
    "# Evaluate again\n",
    "print(reconstructed.evaluate(X_test, y_test, verbose=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the evaluation scores from the original and reconstructed models are\n",
    "tied out perfectly in the last two lines.\n",
    "\n",
    "While pickle is a powerful library, it still does have its own limitations to\n",
    "what can be pickled. For example, live connections such as database connections\n",
    "and opened file handles cannot be pickled. This issue arises because\n",
    "reconstructing these objects requires pickle to re-establish the connection with\n",
    "the database/file, which is something pickle cannot do for you (because it needs\n",
    "appropriate credentials and is out of the scope of what pickle is intended for)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HDF5 in Python\n",
    "\n",
    "Hierarchical Data Format 5 (HDF5) is a binary data format. The h5py package is a\n",
    "Python library that provides an interface to the HDF5 format. From h5py docs,\n",
    "HDF5 “lets you store huge amounts of numerical data, and easily manipulate that\n",
    "data from Numpy.”\n",
    "\n",
    "What HDF5 can do better than other serialization formats is store data in a file\n",
    "system-like hierarchy. You can store multiple objects or datasets in HDF5, like\n",
    "saving multiple files in the file system. You can also read a particular dataset\n",
    "from HDF5, like reading one file from the file system without concerning the\n",
    "other. If you’re using pickle for this, you will need to read and write\n",
    "everything each time you load or create the pickle file. Hence HDF5 is\n",
    "advantageous for huge amounts of data that can’t fit entirely into memory.\n",
    "\n",
    "To get started with h5py, you first need to install the h5py library, which you\n",
    "can do using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /home/juan/.virtualenvs/dl4cv/lib/python3.8/site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/juan/.virtualenvs/dl4cv/lib/python3.8/site-packages (from h5py) (1.21.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get started with creating our first dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    " \n",
    "with h5py.File(\"test.hdf5\", \"w\") as file:\n",
    "    dataset = file.create_dataset(\"test_dataset\", (100,), dtype=\"i4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a new dataset in the file `test.hdf5` named `“test_dataset,”` with a\n",
    "shape of (100, ) and a type int32. `h5py` datasets follow a Numpy syntax so that\n",
    "you can do slicing, retrieval, get shape, etc., similar to Numpy arrays.\n",
    "\n",
    "To retrieve a specific index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dataset identifier (invalid dataset identifier)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2901/949108585.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#retrieves element at index 0 of dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0mnew_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_read_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/functools.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m_fast_read_ok\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;34m\"\"\"Is this dataset suitable for simple reading\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         return (\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extent_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIMPLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh5t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTypeIntegerID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTypeFloatID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3.8/functools.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m_extent_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extent_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;34m\"\"\"Get extent type for this dataset - SIMPLE, SCALAR or NULL\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_simple_extent_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.get_space\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dataset identifier (invalid dataset identifier)"
     ]
    }
   ],
   "source": [
    "dataset[0]  #retrieves element at index 0 of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a slice from index 0 to index 10 of a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialized the h5py file object outside of a with statement, remember to\n",
    "close the file as well!  \n",
    "\n",
    "To read from a previously created HDF5 file, you can\n",
    "open the file in “r” for read mode or “r+” for read/write mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test.hdf5\", \"r\") as file:\n",
    "    print (file.keys()) #gets names of datasets that are in the file\n",
    "    dataset = file[\"test_dataset\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To organize your HDF5 file, you can use groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test.hdf5\", \"w\") as file:\n",
    "    # creates new group_1 in file\n",
    "    file.create_group(\"group_1\")\n",
    "    group1 = file[\"group_1\"]\n",
    "    # creates dataset inside group1\n",
    "    group1.create_dataset(\"dataset1\", shape=(10,))\n",
    "    # to access the dataset\n",
    "    dataset = file[\"group_1\"][\"dataset1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to create groups and files is by specifying the path to the dataset\n",
    "you want to create, and h5py will create the groups on that path as well (if\n",
    "they don’t exist):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test.hdf5\", \"w\") as file:\n",
    "    # creates dataset inside group1\n",
    "    file.create_dataset(\"group1/dataset1\", shape=(10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two snippets of code both create group1 if it has not been created previously and then a dataset1 within group1.\n",
    "\n",
    "## HDF5 in Tensorflow\n",
    "\n",
    "To save a model in Tensorflow Keras using HDF5 format, we can use the `save()` function of the model with a filename having extension .h5, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    " \n",
    "# Create model\n",
    "model = keras.models.Sequential([\n",
    " \tkeras.layers.Input(shape=(10,)),\n",
    " \tkeras.layers.Dense(1)\n",
    "])\n",
    " \n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    " \n",
    "# using the .h5 extension in the file name specifies that the model\n",
    "# should be saved in HDF5 format\n",
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the stored HDF5 model, we can also use the function from Keras directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "model = keras.models.load_model(\"my_model.h5\")\n",
    " \n",
    "# to check that the model has been successfully reconstructed\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason we don’t want to use pickle for a Keras model is that we need a more\n",
    "flexible format that does not tie to a particular version of Keras. If we\n",
    "upgraded our Tensorflow version, the model object might change, and pickle may\n",
    "fail to give us a working model. Another reason is to keep only the essential\n",
    "data for our model. For example, if we check the HDF5 file my_model.h5 created\n",
    "in the above, we see these are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    " \n",
    "with h5py.File(\"my_model.h5\", \"r\") as infile:\n",
    "    print(infile[\"/model_weights/dense/dense/kernel:0\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in HDF5, the metadata is stored alongside the data. Keras stored the\n",
    "network’s architecture in a JSON format in the metadata. Hence we can reproduce\n",
    "our network architecture as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    " \n",
    "with h5py.File(\"my_model.h5\", \"r\") as infile:\n",
    "    for key in infile.attrs.keys():\n",
    "        formatted = infile.attrs[key]\n",
    "        if key.endswith(\"_config\"):\n",
    "            formatted = json.dumps(json.loads(formatted), indent=4)\n",
    "        print(f\"{key}: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model config (i.e., the architecture of our neural network) and training\n",
    "config (i.e., the parameters we passed into the compile() function) are stored\n",
    "as a JSON string. In the code above, we use the json module to reformat it to\n",
    "make it easier to read. It is recommended to save your model as HDF5 rather than\n",
    "just your Python code because, as we can see above, it contains more detailed\n",
    "information than the code on how the network was constructed.  \n",
    "\n",
    "## Comparing Between Different Serialization Methods\n",
    "\n",
    "In the above, we saw how pickle and h5py can help serialize our Python data.\n",
    "\n",
    "We can use pickle to serialize almost any Python object, including user-defined ones and functions. But pickle is not language agnostic. You cannot unpickle it outside Python. There are even 6 versions of pickle developed so far, and older Python may not be able to consume the newer version of pickle data.\n",
    "\n",
    "On the contrary, HDF5 is cross-platform and works well with other language such as Java and C++. In Python, the h5py library implemented the Numpy interface to make it easier to manipulate the data. The data can be accessed in a different language because the HDF5 format supports only the Numpy data types such as float and strings. We cannot store arbitrary objects such as a Python function into HDF5.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "### Articles\n",
    "\n",
    "*    Serialization from C# programming guide, https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/serialization/\n",
    "*    Save and load Keras models, https://www.tensorflow.org/guide/keras/save_and_serialize\n",
    "\n",
    "### Libraries\n",
    "\n",
    "*    pickle, https://docs.python.org/3/library/pickle.html\n",
    "*    h5py, https://docs.h5py.org/en/stable/\n",
    "\n",
    "### APIs\n",
    "\n",
    "*    Tensorflow tf.keras.layers.serialize, https://www.tensorflow.org/api_docs/python/tf/keras/layers/serialize\n",
    "*    Tensorflow tf.keras.models.load_model, https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model\n",
    "*    Tensorflow tf.keras.models.save_model, https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this post, you discovered what serialization is and how to use libraries in Python to serialize Python objects such as dictionaries and Tensorflow Keras models. You have also learned the advantages and disadvantages of two Python libraries for serialization (pickle, h5py).\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "*    what is serialization, and why it is useful\n",
    "*    how to get started with pickle and h5py serialization libraries in Python\n",
    "*    pros and cons of different serialization methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd20b80f3abe9c1782c169f75cb3aaf4fed19688fa30b13c4021414b02823d14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
